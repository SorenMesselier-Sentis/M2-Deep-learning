{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:51.073528Z",
     "start_time": "2025-03-03T13:10:51.070460Z"
    }
   },
   "source": [
    "# Install dependencies\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:52.057018Z",
     "start_time": "2025-03-03T13:10:52.054021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine which device to choose\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "id": "c31f61e3d30303d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:53.772974Z",
     "start_time": "2025-03-03T13:10:53.770574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "data_path = \"data/input.txt\"\n",
    "model_path = \"models/rnn.pth\"\n",
    "tokenizer_path = \"models/tokenizer.pth\"\n",
    "word_tokenizer_path = \"models/word_tokenizer.pth\""
   ],
   "id": "8ea0e0a789bd8ec4",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:55.111531Z",
     "start_time": "2025-03-03T13:10:55.108087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create directories\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ],
   "id": "5330287b34fa4bb7",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:56.709669Z",
     "start_time": "2025-03-03T13:10:56.706109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download Dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "if not os.path.exists(data_path):\n",
    "    response = requests.get(url)\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"Dataset downloaded\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded.\")"
   ],
   "id": "ec75c7c529c5687e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:57.971234Z",
     "start_time": "2025-03-03T13:10:57.968077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Tokenizer character by character\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(set(text))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char2idx = {ch: idx for idx, ch in enumerate(self.chars)}\n",
    "        self.idx2char = {idx: ch for ch, idx in self.char2idx.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return np.array([self.char2idx[ch] for ch in text], dtype=np.int32)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx2char[idx] for idx in indices])\n"
   ],
   "id": "618b30a91663234a",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:10:59.415185Z",
     "start_time": "2025-03-03T13:10:59.411439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Tokenizer word by word\n",
    "class WordTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.words = sorted(set(text.split()))\n",
    "        self.vocab_size = len(self.words)\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return np.array([self.word2idx[word] for word in text.split() if word in self.word2idx], dtype=np.int32)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word[idx] for idx in indices])\n"
   ],
   "id": "e540787e403195bb",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:01.200919Z",
     "start_time": "2025-03-03T13:11:01.163750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and tokenize text\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Initialize tokenizers\n",
    "char_tokenizer = CharTokenizer(text)\n",
    "word_tokenizer = WordTokenizer(text)\n",
    "\n",
    "# Allow switching between tokenizers\n",
    "def get_tokenizer(tokenizer_type):\n",
    "    if tokenizer_type == \"char\":\n",
    "        return char_tokenizer\n",
    "    elif tokenizer_type == \"word\":\n",
    "        return word_tokenizer\n",
    "    else:\n",
    "        raise ValueError(\"Unknown tokenizer type. Use 'char' or 'word'.\")\n"
   ],
   "id": "410a949e063c9625",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:03.700560Z",
     "start_time": "2025-03-03T13:11:03.696950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Switch between \"char\" and \"word\" tokenizer\n",
    "tokenizer_type = \"word\"\n",
    "tokenizer = get_tokenizer(tokenizer_type)"
   ],
   "id": "3078aaf9609729d4",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:04.896150Z",
     "start_time": "2025-03-03T13:11:04.845096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize the text by word\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Initialize tokenizer\n",
    "word_tokenizer = WordTokenizer(text)\n",
    "torch.save(word_tokenizer, tokenizer_path)\n",
    "print(\"word tokenizer saved\")"
   ],
   "id": "26c9128cce48471b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word tokenizer saved\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:08.976383Z",
     "start_time": "2025-03-03T13:11:08.971757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Dataset Class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, seq_length=100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.tokenizer.encode(text)\n",
    "        self.seq_length = seq_length\n",
    "        self.num_samples = len(self.data) - seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx:idx+self.seq_length]\n",
    "        target_seq = self.data[idx+1:idx+self.seq_length+1]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n"
   ],
   "id": "9e566ca9a4781ce",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:11.554563Z",
     "start_time": "2025-03-03T13:11:11.551210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Model\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(2, batch_size, 256), torch.zeros(2, batch_size, 256))\n"
   ],
   "id": "cced34d20e116a21",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:11:14.061038Z",
     "start_time": "2025-03-03T13:11:14.056286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the Model\n",
    "def train_model(model, data, vocab_size, optimizer, criterion, device, writer, num_epochs=0, seq_length=100):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        hidden = None\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, len(data) - seq_length, seq_length):\n",
    "            inputs = torch.tensor(data[i:i+seq_length], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            targets = torch.tensor(data[i+1:i+seq_length+1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            if hidden is not None:\n",
    "                if isinstance(hidden, tuple):\n",
    "                    hidden = tuple(h.detach() for h in hidden)\n",
    "                else:\n",
    "                    hidden = hidden.detach()\n",
    "\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            writer.add_scalar(\"Loss/Batch\", loss.item(), global_step)\n",
    "            global_step += 1\n",
    "        avg_loss = epoch_loss / ((len(data) - seq_length) // seq_length)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Loss/Epoch\", avg_loss, epoch+1)\n",
    "    return model"
   ],
   "id": "64c48c8e7ea93f2b",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:20:13.383985Z",
     "start_time": "2025-03-03T13:11:16.269393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Setup\n",
    "writer = SummaryWriter(\"logs\")\n",
    "\n",
    "model = CharRNN(tokenizer.vocab_size).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data = tokenizer.encode(text)\n",
    "model = train_model(model, data, tokenizer.vocab_size, optimizer, criterion, device, writer)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved\")"
   ],
   "id": "34f0eded70be1538",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 7.6472\n",
      "Epoch 2/20, Loss: 6.6793\n",
      "Epoch 3/20, Loss: 6.1535\n",
      "Epoch 4/20, Loss: 5.7276\n",
      "Epoch 5/20, Loss: 5.3709\n",
      "Epoch 6/20, Loss: 4.9992\n",
      "Epoch 7/20, Loss: 4.6226\n",
      "Epoch 8/20, Loss: 4.2884\n",
      "Epoch 9/20, Loss: 3.9874\n",
      "Epoch 10/20, Loss: 3.7333\n",
      "Epoch 11/20, Loss: 3.4798\n",
      "Epoch 12/20, Loss: 3.2601\n",
      "Epoch 13/20, Loss: 3.0592\n",
      "Epoch 14/20, Loss: 2.8904\n",
      "Epoch 15/20, Loss: 2.7204\n",
      "Epoch 16/20, Loss: 2.5651\n",
      "Epoch 17/20, Loss: 2.4176\n",
      "Epoch 18/20, Loss: 2.2844\n",
      "Epoch 19/20, Loss: 2.1551\n",
      "Epoch 20/20, Loss: 2.0467\n",
      "Model saved\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:21:00.636535Z",
     "start_time": "2025-03-03T13:21:00.634014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate Text\n",
    "def generate_text(start_string, length=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor(word_tokenizer.encode(start_string), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "    output_text = start_string\n",
    "\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "        output_dist = output[:, -1, :] / temperature\n",
    "        predicted_id = torch.multinomial(torch.softmax(output_dist, dim=1), 1).item()\n",
    "        input_seq = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "        output_text += \" \" + word_tokenizer.decode([predicted_id])\n",
    "\n",
    "    return output_text"
   ],
   "id": "884a494cdb92a837",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T13:21:03.962033Z",
     "start_time": "2025-03-03T13:21:02.905210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate sample text\n",
    "print(generate_text(\"ROMEO:\", length=500, temperature=0.8))"
   ],
   "id": "982a75511734f45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Who left forgot As the grief doth usurp the bigger light, and a sort. ANTONIO: A laughter. SEBASTIAN: Upon the acorn cradled. Follow. his friends, loss, As if a spendthrift of the merchant And burn in their King of Tunis. ANTONIO: No marrying 'mong the maid: your king's loss, and relieve him, indeed: you not your men that's not a man than the miraculous harp; it is not a prison. GONZALO: Come, sir, I speak not to the entertainer-- SEBASTIAN: Which is a man of Milan, he's a traitor. Come; I are not in all the breasts breathes again or endeavour: treason, felony, Sword, pike, knife, gun, or need for ever: Milan and Naples Have about their rotten ones. CATESBY: We are made home to revenge the bloody colours of ADRIAN: GONZALO: No, my most man is gone with all the rest of green too. SEBASTIAN: Is the miracle, of this business' use he of joy; or dost thou waking? SEBASTIAN: Good the ground for her rats are in thirst a silly thing, The king's cock. ANTONIO: The cockerel. SEBASTIAN: Done. The wager? ANTONIO: A laughter. SEBASTIAN: Who receives drunk the most rich ADRIAN: Yet,-- ADRIAN: Yet,-- ANTONIO: Of what it is my queen. ANTONIO: That Comes doth open him all the miraculous harp; he kept, and oar'd Himself with mine arm it o'er some delicate ANTONIO: The other of The commonwealth I have a good man. But with a King of Naples hath too delicate flat-long. GONZALO: Sir, he does speak of Milan, more days of my sense. Come to the syllable. ANTONIO: A laughter. SEBASTIAN: Not a verdict? Which with such delicate ADRIAN: Tunis of the air breathes out Of Naples Will bring a Europe to be desert,-- GONZALO: hint of SEBASTIAN: Is such vile comfort. ANTONIO: My lord Sebastian, ANTONIO: O ho, O tribunes of my King FERDINAND: Under it again; I am too bold it in: you rub the sore, When you may blow it, 'tis not at the queen. ANTONIO: The cockerel. SEBASTIAN: Done. The wager? ANTONIO: The belly answer'd-- Come ye most falsely ANTONIO: My widow more than the affections to this marriage of my rate, he hath been so stinging as stained for the commonwealth I shall uneasy more islands. GONZALO: Good Lord, Carthage. SEBASTIAN: GONZALO: And,--do my lord,-- ANTONIO: Fie, sow't a vice in thine own price. natural or mallows. GONZALO: I may make myself To come to prison. SEBASTIAN: No; if I Let us had almost beyond a prison. CALIBAN: No, come; I have lost my knee and not a kind of Naples than most delicate ADRIAN: Tunis is indeed almost inaccessible,-- SEBASTIAN: Yet,-- ANTONIO: The belly answer'd-- Of thine man's innocent nature, you must needs first do not strike, what I use here see that thou dost met it not again i' the earth Let me relieve thee of our infancy your loss, SEBASTIAN: One: tell. GONZALO: I have hearts a fresh nature, As matter i' the earth, for widow\n"
     ]
    }
   ],
   "execution_count": 94
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
